{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"PYTHONPATH: {}\".format(os.environ['PYTHONPATH']))\n",
    "print(\"Spark: {}\".format(spark.version))\n",
    "print(\"Python: {}\".format(sys.version))\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leo, guardo y consulto tabla externa Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.DataFrame({\"day\": ['mon', 'tue', 'wed', 'fri'], \n",
    "                    \"sales\": [5, 1, 2, 3]})\n",
    "df = spark.createDataFrame(days)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escribo datos en parquet \n",
    "dataDir = \"/tmp/days_parquet\"\n",
    "df.write.parquet(dataDir, mode=\"overwrite\")\n",
    "\n",
    "# creo tabla externa\n",
    "spark.sql(\"DROP TABLE IF EXISTS days_ext\")\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE days_ext (day string, sales bigint) STORED AS PARQUET LOCATION '{dataDir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# consulto la tabla \n",
    "spark.sql(\"SELECT * FROM days_ext\").orderBy('day').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df2 = df.select('day', (2 * col('sales')).alias('sales'))\n",
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.parquet(dataDir, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consulto la tabla \n",
    "spark.sql(\"SELECT * FROM days_ext\").orderBy('day').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS days_ext\")\n",
    "spark.sql('show tables').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r -f -skipTrash /tmp/days_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar DataFrame como Tabla Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_dom = spark.sparkContext.textFile(\"/data/CaliforniaHousing/cal_housing.domain\")\\\n",
    "    .map(lambda s: s.split(':')[0])\n",
    "columns = house_dom.collect()\n",
    "\n",
    "house_data = spark.read.csv(\"/data/CaliforniaHousing/cal_housing.data\", inferSchema=True)\\\n",
    "    .toDF(*columns)\n",
    "house_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM housing\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar Dataframe como TempView y guardar con CTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"key\", \"value\")\n",
    "records_df = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 5)])\n",
    "records_df.createOrReplaceTempView(\"records_view\")\n",
    "spark.sql(\"SELECT * FROM records_view\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r -f -skipTrash /tmp/records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE EXTERNAL TABLE records STORED AS orc LOCATION '/tmp/records' AS SELECT * FROM records_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM records\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saveAsTable\n",
    "\n",
    "Recomiendo no usar pq es inestable y da error en varias situaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3Spark3",
   "language": "python",
   "name": "py3spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
