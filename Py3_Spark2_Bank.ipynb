{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark.implicits._\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "bank_txt = spark.sparkContext.textFile(\"/data/examples/bank.csv\")\n",
    "\n",
    "bank_df = bank_txt.map(lambda s: re.sub(r'(?:^\\\"|\\\"$)', '', s))\\\n",
    "    .map(lambda s: re.split('\\\"?;\\\"?', s))\\\n",
    "    .filter(lambda s: s[0] != \"age\")\\\n",
    "    .map(lambda s: [int(s[0])] + s[1:4] + [int(s[5])]).toDF(['age', 'job', 'marital', 'education', 'balance'])\n",
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.createOrReplaceTempView('bank_view')\n",
    "bank_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pdf = spark.sql('select education, marital, count(*) as count from bank_view group by education,marital')\\\n",
    "    .toPandas()\n",
    "em_pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = em_pdf.pivot(index='education', columns='marital', values='count')\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(kind='bar', stacked=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/data/examples/bank.csv\", header=True, sep=';', \n",
    "                      mode=\"DROPMALFORMED\", inferSchema=True)\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"default.bank\");\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spark.catalog.listTables(\"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = spark.read.table(\"bank\")\n",
    "bank_jobs = bank_df.groupBy(\"job\").count()\n",
    "bank_jobs.createOrReplaceTempView(\"bank_jobs\")\n",
    "jobs_pd = spark.sql(\"select * from bank_jobs order by count desc limit 10\").toPandas()\n",
    "jobs_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_pd.plot(kind='bar', x='job', y='count', stacked=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Hive using HWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_llap import HiveWarehouseSession\n",
    "hive = HiveWarehouseSession.session(spark).build()\n",
    "hive.setDatabase(\"default\")\n",
    "hive.showTables().limit(10).toPandas()\n",
    "hive.dropTable('bank', True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.write.format(HiveWarehouseSession().HIVE_WAREHOUSE_CONNECTOR)\\\n",
    "    .mode(\"overwrite\").option(\"table\", \"bank\").save()\n",
    "df = hive.executeQuery(\"select * from bank\")\n",
    "df.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3Spark2",
   "language": "python",
   "name": "py3spark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
